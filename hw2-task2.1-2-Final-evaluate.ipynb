{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OyQOo0f65daB"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# import spacy\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "import collections\n",
    "import pickle\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from torch.utils.data import DataLoader, Dataset, SequentialSampler, RandomSampler\n",
    "import torch.nn.utils.rnn as rnn_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tx8u7Nsf63L5"
   },
   "outputs": [],
   "source": [
    "data_dir = './data/'\n",
    "proj_dir = './'\n",
    "class Vocabulary(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.word2idx = {'<SOS>': 0, '<EOS>': 1, '<PAD>': 2, '<UNK>': 3}\n",
    "        self.idx2word = {0: '<SOS>', 1: '<EOS>', 2: '<PAD>', 3: '<UNK>'}\n",
    "        self.num_words = 4\n",
    "        self.OOV_list = []\n",
    "        self.OOV=0      \n",
    "\n",
    "    def build_vocab(self, data_path):\n",
    "        \"\"\"Construct the relation between words and indices\"\"\"\n",
    "        i=0\n",
    "        with open(data_path, 'r', encoding='utf-8') as dataset:\n",
    "            for words in dataset:\n",
    "                parts = words.strip('\\n').strip().split('\\t')\n",
    "                assert len(parts) == 2, 'Not 2 parts'+words\n",
    "                words = parts[0].split()\n",
    "                if i<5:\n",
    "                    print(words)\n",
    "                i+=1\n",
    "                for word in words:\n",
    "                    if word not in self.word2idx:\n",
    "                        self.word2idx[word] = self.num_words\n",
    "                        self.idx2word[self.num_words] = word\n",
    "                        self.num_words += 1\n",
    "\n",
    "    def sequence_to_indices(self, sequence, add_eos=False, add_sos=False):\n",
    "        \"\"\"Transform a char sequence to index sequence\n",
    "            :param sequence: a string composed with chars\n",
    "            :param add_eos: if true, add the <EOS> tag at the end of given sentence\n",
    "            :param add_sos: if true, add the <SOS> tag at the beginning of given sentence\n",
    "        \"\"\"\n",
    "        index_sequence = [self.word2idx['<SOS>']] if add_sos else []\n",
    "\n",
    "        for word in sequence:\n",
    "            if word not in self.word2idx:\n",
    "                self.OOV+=1\n",
    "                self.OOV_list.append(word)\n",
    "                index_sequence.append((self.word2idx['<UNK>']))\n",
    "            else:\n",
    "                index_sequence.append(self.word2idx[word])\n",
    "\n",
    "        if add_eos:\n",
    "            index_sequence.append(self.word2idx['<EOS>'])\n",
    "\n",
    "        return index_sequence\n",
    "\n",
    "    def indices_to_sequence(self, indices,print_signal=False):\n",
    "        \"\"\"Transform a list of indices\n",
    "            :param indices: a list\n",
    "        \"\"\"\n",
    "        sequence = []\n",
    "        for idx in indices:\n",
    "            word = self.idx2word[idx]\n",
    "            if word == \"<EOS>\" and (not print_signal):\n",
    "                sequence.append(word)\n",
    "                break\n",
    "            elif word == '<PAD>':\n",
    "                break\n",
    "            else:\n",
    "                sequence.append(word)\n",
    "        return sequence\n",
    "\n",
    "    def __str__(self):\n",
    "        str = \"Vocab information:\\n\"\n",
    "        for idx, word in self.idx2word.items():\n",
    "            str += \"word: %s Index: %d\\n\" % (word, idx)\n",
    "        return str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jKj7rJox7S4S"
   },
   "outputs": [],
   "source": [
    "class MyData(Dataset):\n",
    "    def __init__(self, path,vocab):\n",
    "        self.src_indices_seq = []\n",
    "        self.trg_indices_seq = []\n",
    "        self.vocab = vocab\n",
    "        self.PAD_ID = self.vocab.word2idx[\"<PAD>\"]\n",
    "        self.SOS_ID = self.vocab.word2idx[\"<SOS>\"]\n",
    "        self.vocab_size = self.vocab.num_words\n",
    "        self.max_length = -1\n",
    "        self.raw_src_sent_data = [] #src sentence\n",
    "        self.raw_trg_sent_data = [] #target sentence\n",
    "#         self.max_length = self.vocab.max_length\n",
    "        c=0\n",
    "        file=open(path, 'r', encoding='utf-8')\n",
    "        for line in file:\n",
    "            pparts = line.strip('\\n').split('\\t')\n",
    "            assert len(pparts) == 2, 'Error!!'\n",
    "            words,trg_sentence = pparts\n",
    "            trg_sentence = trg_sentence.split()\n",
    "            self.raw_trg_sent_data.append(trg_sentence)\n",
    "            if trg_sentence[0]!='<SOS>':\n",
    "                print('trg:',trg_sentence)\n",
    "            if self.max_length < len(trg_sentence):\n",
    "                self.max_length = len(trg_sentence)\n",
    "            src_sent = words.strip('\\n').split()\n",
    "            self.raw_src_sent_data.append(src_sent)\n",
    "            if src_sent[0]!='<SOS>':\n",
    "                print('src:',src_sent)\n",
    "            if self.max_length < len(src_sent):\n",
    "                self.max_length = len(src_sent)\n",
    "        assert len(self.raw_trg_sent_data)==len(self.raw_src_sent_data),'Error 2!'+line\n",
    "        for i,trg_sent in enumerate(self.raw_trg_sent_data):\n",
    "            indices_seq = self.vocab.sequence_to_indices(self.raw_src_sent_data[i], add_eos=False)\n",
    "            self.src_indices_seq.append(torch.tensor(indices_seq))\n",
    "            indices_seq = self.vocab.sequence_to_indices(trg_sent, add_eos=False)\n",
    "            self.trg_indices_seq.append(torch.tensor(indices_seq))\n",
    "        self.src_indices_seq = rnn_utils.pad_sequence(self.src_indices_seq, batch_first=True, padding_value=self.PAD_ID)\n",
    "        self.trg_indices_seq = rnn_utils.pad_sequence(self.trg_indices_seq, batch_first=True, padding_value=self.PAD_ID)\n",
    "        print(\"## J: Total examples: %d, unique words:%d, Max seq length: %d\"%(len(self.src_indices_seq),self.vocab_size,self.max_length))\n",
    "    # def collate_fn(data):\n",
    "    #     data = rnn_utils.pad_sequence(data, batch_first=True, padding_value=0)\n",
    "    #     return data\n",
    "    def __len__(self):\n",
    "        return len(self.src_indices_seq)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.src_indices_seq[idx]), torch.tensor(self.trg_indices_seq[idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ts8gDQj_G0WR"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yqD0fuycHB58"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        \n",
    "        # self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
    "        self.gru = nn.GRU(emb_dim, hid_dim, n_layers,batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layerN = nn.LayerNorm(emb_dim)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \n",
    "        #src = [src sent len, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.layerN((self.embedding(src))))\n",
    "#         embedded = self.dropout(src)\n",
    "        \n",
    "        #embedded = [src sent len, batch size, emb dim]\n",
    "        \n",
    "        # outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        outputs, hidden = self.gru(embedded)\n",
    "        \n",
    "        #outputs = [src sent len, batch size, hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #outputs are always from the top hidden layer\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.output_dim = output_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        \n",
    "        # self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
    "        self.gru = nn.GRU(emb_dim, hid_dim, n_layers,batch_first=True)\n",
    "        \n",
    "        self.out = nn.Linear(hid_dim, output_dim)\n",
    "        \n",
    "        self.layerN_emb = nn.LayerNorm(emb_dim)\n",
    "        self.layerN = nn.LayerNorm(hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.dp_dense = nn.Dropout(0.1)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        \n",
    "        #input = [batch size]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #n directions in the decoder will both always be 1, therefore:\n",
    "        #hidden = [n layers, batch size, hid dim]\n",
    "        #context = [n layers, batch size, hid dim]\n",
    "        \n",
    "        input = input.unsqueeze(1)\n",
    "        \n",
    "#         #input = [1, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.layerN_emb(self.embedding(input)))\n",
    "#         embedded = self.dropout(input)\n",
    "        \n",
    "        #embedded = [1, batch size, emb dim]\n",
    "                \n",
    "        # output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        output, hidden = self.gru(embedded,hidden)\n",
    "        \n",
    "        #output = [sent len, batch size, hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, hid dim]\n",
    "        #cell = [n layers * n directions, batch size, hid dim]\n",
    "        \n",
    "        #sent len and n directions will always be 1 in the decoder, therefore:\n",
    "        #output = [1, batch size, hid dim]\n",
    "        #hidden = [n layers, batch size, hid dim]\n",
    "        #cell = [n layers, batch size, hid dim]\n",
    "        prediction = self.layerN(output.squeeze(1))\n",
    "        prediction = self.out(self.dp_dense(prediction))\n",
    "        \n",
    "        #prediction = [batch size, output dim]\n",
    "        \n",
    "        return prediction, hidden\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "#         self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
    "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "        assert encoder.n_layers == decoder.n_layers, \\\n",
    "            \"Encoder and decoder must have equal number of layers!\"\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
    "        \n",
    "        #src = [src sent len, batch size]\n",
    "        #trg = [trg sent len, batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
    "        \n",
    "        batch_size = trg.shape[0]\n",
    "        max_len = trg.shape[1]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "#         outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        outputs = torch.zeros(batch_size, max_len, trg_vocab_size).to(self.device)\n",
    "        outputs_idx = torch.zeros(batch_size, max_len-1)\n",
    "\n",
    "        #last hidden state of the encoder is used as the initial hidden state of the decoder\n",
    "        hidden = self.encoder(src)\n",
    "#         embedded = self.embedding(src)\n",
    "#         hidden = self.encoder(embedded)\n",
    "        \n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        input = trg[:,0]\n",
    "        \n",
    "        for t in range(1, max_len):\n",
    "            \n",
    "            #insert input token embedding, previous hidden and previous cell states\n",
    "            #receive output tensor (predictions) and new hidden and cell states\n",
    "            output, hidden= self.decoder(input, hidden)\n",
    "#             input = input.unsqueeze(1)\n",
    "#             embedded = self.embedding(input)\n",
    "#             output, hidden= self.decoder(embedded, hidden)\n",
    "            \n",
    "            #place predictions in a tensor holding predictions for each token\n",
    "            outputs[:,t,:] = output\n",
    "            \n",
    "            #decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            #get the highest predicted token from our predictions\n",
    "            top1 = output.argmax(1)\n",
    "            outputs_idx[:,t-1] = top1\n",
    "            \n",
    "            #if teacher forcing, use actual next token as next input\n",
    "            #if not, use predicted token\n",
    "            input = trg[:,t] if teacher_force else top1\n",
    "        \n",
    "        return outputs,outputs_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WEnMgjnlOATV"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm, trange\n",
    "def decode_string(output,dataset):\n",
    "    results = []\n",
    "    trg = []\n",
    "    correct = 0\n",
    "    for i,seq in enumerate(output):\n",
    "        results.append(' '.join(['<SOS>']+dataset.vocab.indices_to_sequence(seq)))\n",
    "        trg.append(' '.join(dataset.raw_trg_sent_data[i]))\n",
    "        if results[-1] == trg[-1]:\n",
    "            correct+=1\n",
    "        \n",
    "    print('-----exm-----')\n",
    "    print(results[:10])\n",
    "    print(trg[:10])\n",
    "    return results, correct/len(output)\n",
    "\n",
    "def evaluate(model,criterion,dataset):\n",
    "    prediction = []\n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    data_loader = DataLoader(dataset, batch_size=512, shuffle=False)\n",
    "    trange = tqdm(enumerate(data_loader), total=len(data_loader),desc='valid')\n",
    "    for step, batch in trange:\n",
    "        src = batch[0]\n",
    "        trg = batch[1]\n",
    "        src = src.to(device)\n",
    "        trg = trg.to(device)\n",
    "        with torch.no_grad():\n",
    "            output,output_idx = model(src, trg, 0) #turn off teacher forcing\n",
    "            prediction.append(output_idx.to('cpu'))\n",
    "\n",
    "            #trg = [trg sent len, batch size]\n",
    "            #output = [trg sent len, batch size, output dim]\n",
    "\n",
    "            output = output[:,1:].reshape(-1, output.shape[-1])\n",
    "            trg = trg[:,1:].reshape(-1)\n",
    "\n",
    "            #trg = [(trg sent len - 1) * batch size]\n",
    "            #output = [(trg sent len - 1) * batch size, output dim]\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "    prediction = torch.cat(prediction).detach().numpy().astype(int)\n",
    "    prediction,acc = decode_string(prediction,dataset)\n",
    "    return epoch_loss / len(data_loader),prediction,acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "valid:   0%|          | 0/48 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## J: Total examples: 24510, unique words:198, Max seq length: 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "valid: 100%|██████████| 48/48 [00:01<00:00, 28.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----exm-----\n",
      "['<SOS> do , <EOS>', '<SOS> tom <EOS>', '<SOS> to mary so <EOS>', '<SOS> said would do , mary <EOS>', '<SOS> do so <EOS>', '<SOS> asked asked , do would said <EOS>', '<SOS> much time i <EOS>', '<SOS> do <EOS>', '<SOS> how more time <EOS>', '<SOS> i time <EOS>']\n",
      "[\"<SOS> n't that <EOS>\", '<SOS> tom <EOS>', '<SOS> mary i , <EOS>', '<SOS> said would do , i <EOS>', '<SOS> do , <EOS>', '<SOS> mary i , do would said <EOS>', '<SOS> much time i <EOS>', '<SOS> do <EOS>', '<SOS> how more do <EOS>', '<SOS> i time <EOS>']\n",
      "24510 0.9336597307221542\n"
     ]
    }
   ],
   "source": [
    "vocab = Vocabulary()\n",
    "vocab_dic = pickle.load(open('./output_final/vocab_final.pkl','rb'))\n",
    "vocab.word2idx = vocab_dic['word2idx']; vocab.idx2word = vocab_dic['idx2word']\n",
    "vocab.num_words = vocab_dic['num_words']\n",
    "model_load = torch.load(open('./output_final/model_final_9336.pkl','rb'))\n",
    "test_set = MyData(data_dir+'final_data/val.txt',vocab=vocab)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = test_set.PAD_ID)\n",
    "valid_loss,valid_predict,valid_acc = evaluate(model_load, criterion,test_set)\n",
    "print(len(valid_predict),valid_acc)\n",
    "# f_out = open('./output/task2_1_1_sample_predictions.txt','w')\n",
    "# for row in valid_predict:\n",
    "#     f_out.write(row+'\\n')\n",
    "# f_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98040\n",
      "0.7814973480212158\n"
     ]
    }
   ],
   "source": [
    "print(len(valid_predict))\n",
    "print(valid_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
